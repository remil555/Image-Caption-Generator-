{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835},{"sourceId":343604,"sourceType":"datasetVersion","datasetId":145129}],"dockerImageVersionId":29855,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\nfrom keras.layers import concatenate, BatchNormalization, Input\nfrom keras.layers.merge import add\nfrom keras.utils import to_categorical\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport string\nimport time\nprint(\"Running.....\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_path = '/kaggle/input/flickr8k/flickr_data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\ntext = open(token_path, 'r', encoding = 'utf-8').read()\nprint(text[:500])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_description(text):\n    mapping = dict()\n    for line in text.split(\"\\n\"):\n        token = line.split(\"\\t\")\n        if len(line) < 2:\n            continue\n        img_id = token[0].split('.')[0]\n        img_des = token[1]\n        if img_id not in mapping:\n            mapping[img_id] = list()\n        mapping[img_id].append(img_des)\n    return mapping\n\ndescriptions = load_description(text)\nprint(\"Number of items: \" + str(len(descriptions)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions['1000268201_693b08cb0e']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_description(desc):\n    for key, des_list in desc.items():\n        for i in range(len(des_list)):\n            caption = des_list[i]\n            caption = [ch for ch in caption if ch not in string.punctuation]\n            caption = ''.join(caption)\n            caption = caption.split(' ')\n            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n            caption = ' '.join(caption)\n            des_list[i] = caption\n\nclean_description(descriptions)\ndescriptions['1000268201_693b08cb0e']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_vocab(desc):\n    words = set()\n    for key in desc.keys():\n        for line in desc[key]:\n            words.update(line.split())\n    return words\nvocab = to_vocab(descriptions)\nlen(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimages = '/kaggle/input/flickr8k/flickr_data/Flickr_Data/Images/'\n# Create a list of all image names in the directory\nimg = glob.glob(images + '*.jpg')\nlen(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/flickr8k/flickr_data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\ntrain_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\ntrain_img = []\n\nfor im in img:\n    if(im[len(images):] in train_images):\n        train_img.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '/kaggle/input/flickr8k/flickr_data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'\ntest_images = open(test_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\ntest_img = []\n\nfor im in img:\n    if(im[len(images): ] in test_images):\n        test_img.append(im)\nlen(test_img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load descriptions of train and test set separately\ndef load_clean_descriptions(des, dataset):\n    dataset_des = dict()\n    for key, des_list in des.items():\n        if key+'.jpg' in dataset:\n            if key not in dataset_des:\n                dataset_des[key] = list()\n            for line in des_list:\n                desc = 'startseq ' + line + ' endseq'\n                dataset_des[key].append(desc)\n    return dataset_des\n\ntrain_descriptions = load_clean_descriptions(descriptions, train_images)\nprint('Descriptions: train=%d' % len(train_descriptions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_descriptions['1000268201_693b08cb0e']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\ndef preprocess_img(img_path):\n    #inception v3 excepts img in 299*299\n    img = load_img(img_path, target_size = (299, 299))\n    x = img_to_array(img)\n    # Add one more dimension\n    x = np.expand_dims(x, axis = 0)\n    x = preprocess_input(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = InceptionV3(weights = 'imagenet')\nbase_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(base_model.input, base_model.layers[-2].output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to encode an image into a vector using inception v3\ndef encode(image):\n    image = preprocess_img(image)\n    vec = model.predict(image)\n    vec = np.reshape(vec, (vec.shape[1]))\n    return vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run the encode function on all train images\nstart = time.time()\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images):]] = encode(img)\nprint(\"Time Taken is: \" + str(time.time() - start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encode all the test images\nstart = time.time()\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images):]] = encode(img)\nprint(\"Time taken is: \" + str(time.time() - start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = encoding_train\ntest_features = encoding_test\nprint(\"Train image encodings: \" + str(len(train_features)))\nprint(\"Test image encodings: \" + str(len(test_features)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features['1000268201_693b08cb0e.jpg'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list of all training captions\nall_train_captions = []\nfor key, val in train_descriptions.items():\n    for caption in val:\n        all_train_captions.append(caption)\nlen(all_train_captions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#onsider only words which occur atleast 10 times\nvocabulary = vocab\nthreshold = 10\nword_counts = {}\nfor cap in all_train_captions:\n    for word in cap.split(' '):\n        word_counts[word] = word_counts.get(word, 0) + 1\n\nvocab = [word for word in word_counts if word_counts[word] >= threshold]\nprint(\"Unique words: \" + str(len(word_counts)))\nprint(\"our Vocabulary: \" + str(len(vocab)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word mapping to integers\nixtoword = {}\nwordtoix = {}\n\nix = 1\nfor word in vocab:\n    wordtoix[word] = ix\n    ixtoword[ix] = word\n    ix += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(ixtoword) + 1  #1 for appended zeros\nvocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find the maximum length of a description in a dataset\nmax_length = max(len(des.split()) for des in all_train_captions)\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#since there are almost 30000 descriptions to process we will use datagenerator\nX1, X2, y = list(), list(), list()\nfor key, des_list in train_descriptions.items():\n    pic = train_features[key + '.jpg']\n    for cap in des_list:\n        seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n            #store\n            X1.append(pic)\n            X2.append(in_seq)\n            y.append(out_seq)\n\nX2 = np.array(X2)\nX1 = np.array(X1)\ny = np.array(y)\nprint(X1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load glove vectors for embedding layer\nembeddings_index = {}\nglove = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt', 'r', encoding = 'utf-8').read()\nfor line in glove.split(\"\\n\"):\n    values = line.split(\" \")\n    word = values[0]\n    indices = np.asarray(values[1: ], dtype = 'float32')\n    embeddings_index[word] = indices\nprint('Total word vectors: ' + str(len(embeddings_index)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_dim = 200\nemb_matrix = np.zeros((vocab_size, emb_dim))\nfor word, i in wordtoix.items():\n    emb_vec = embeddings_index.get(word)\n    if emb_vec is not None:\n        emb_matrix[i] = emb_vec\nemb_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\nip1 = Input(shape = (2048, ))\nfe1 = Dropout(0.2)(ip1)\nfe2 = Dense(256, activation = 'relu')(fe1)\nip2 = Input(shape = (max_length, ))\nse1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)\nse2 = Dropout(0.2)(se1)\nse3 = LSTM(256)(se2)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation = 'relu')(decoder1)\noutputs = Dense(vocab_size, activation = 'softmax')(decoder2)\nmodel = Model(inputs = [ip1, ip2], outputs = outputs)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.layers[2].set_weights([emb_matrix])\nmodel.layers[2].trainable = False\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\nplot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    model.fit([X1, X2], y, epochs = 1, batch_size = 256)\n    if(i%2 == 0):\n        model.save_weights(\"image-caption-weights\" + str(i) + \".h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_search(pic):\n    start = 'startseq'\n    for i in range(max_length):\n        seq = [wordtoix[word] for word in start.split() if word in wordtoix]\n        seq = pad_sequences([seq], maxlen = max_length)\n        yhat = model.predict([pic, seq])\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        start += ' ' + word\n        if word == 'endseq':\n            break\n    final = start.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = list(encoding_test.keys())[250]\nimg = encoding_test[pic].reshape(1, 2048)\nx = plt.imread(images + pic)\nplt.imshow(x)\nplt.show()\nprint(greedy_search(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = list(encoding_test.keys())[570]\nimg = encoding_test[pic].reshape(1, 2048)\nx = plt.imread(images + pic)\nplt.imshow(x)\nplt.show()\nprint(greedy_search(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"my_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train it for some more time\nmodel.fit([X1, X2], y, epochs = 1, batch_size = 64)\nmodel.save(\"my_model_\"+str(i)+\".h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pic = list(encoding_test.keys())[888]\nimg = encoding_test[pic].reshape(1, 2048)\nx = plt.imread(images + pic)\nplt.imshow(x)\nplt.show()\nprint(greedy_search(img))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"my-cap.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}